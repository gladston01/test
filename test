import pandas as pd
import os
import json
import argparse
from datetime import datetime
import psycopg2

# PostgreSQL Database Configuration
DB_CONFIG = {
    "dbname": "performance_db",
    "user": "username",
    "password": "password",
    "host": "localhost",
    "port": 5432,
}

# Thresholds for detecting spikes and anomalies
MESSAGE_SPIKE_THRESHOLD = 200  # Duration in ms to classify a spike
CPU_THRESHOLD = 50  # Percentage
MEMORY_THRESHOLD = 50  # Percentage
TPS_THRESHOLD = 300  # Transactions per second
WORKLIST_THRESHOLD = 800
LOAD_THRESHOLD = 80

# Step 1: Fetch Data from PostgreSQL
def fetch_data(start_time, end_time):
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        
        # Queries to fetch data
        durations_query = f"""
        SELECT dpbox, leg, domain, service, messagedurations, requestdurations, responsedurations, serverdurations, timestamp
        FROM dpmessagedurations
        WHERE timestamp BETWEEN '{start_time}' AND '{end_time}';
        """
        counts_query = f"""
        SELECT dpbox, leg, domain, service, errormessagecount, requestmessagecount, rresponsemessagecount, timestamp
        FROM dpmessagecounts
        WHERE timestamp BETWEEN '{start_time}' AND '{end_time}';
        """
        metrics_query = f"""
        SELECT dpbox, leg, domain, cpu, worklist, memory, load, timestamp
        FROM dpsystemmetrics
        WHERE timestamp BETWEEN '{start_time}' AND '{end_time}';
        """
        
        # Read data into Pandas DataFrames
        durations_df = pd.read_sql_query(durations_query, conn)
        counts_df = pd.read_sql_query(counts_query, conn)
        metrics_df = pd.read_sql_query(metrics_query, conn)
        
        conn.close()
        return durations_df, counts_df, metrics_df
    except Exception as e:
        print(f"Error fetching data: {e}")
        return None, None, None

# Step 2: Merge Data from Multiple Tables
def merge_data(durations_df, counts_df, metrics_df):
    # Merge durations and counts on all common columns
    merged_df = pd.merge(
        durations_df, 
        counts_df, 
        on=['dpbox', 'leg', 'domain', 'service', 'timestamp'], 
        how='inner'
    )

    # Merge with metrics table (exclude 'service' as it does not exist in the metrics table)
    merged_df = pd.merge(
        merged_df, 
        metrics_df, 
        on=['dpbox', 'leg', 'domain', 'timestamp'], 
        how='inner'
    )
    
    # Handle missing values
    merged_df = merged_df.fillna(0)
    return merged_df

# Step 3: Detect Spikes
def detect_spikes(df):
    spikes = []
    spike_active = False
    spike_start = None
    spike_metrics = None

    for i, row in df.iterrows():
        if row['messagedurations'] > MESSAGE_SPIKE_THRESHOLD:
            if not spike_active:
                spike_active = True
                spike_start = row['timestamp']
                spike_metrics = row  # Capture initial metrics
            else:
                spike_metrics = row.combine(spike_metrics, max)
        else:
            if spike_active:  # End the spike
                spike_active = False
                spike_end = row['timestamp']
                spikes.append({
                    "start_time": spike_start,
                    "end_time": spike_end,
                    "dpbox": spike_metrics['dpbox'],
                    "domain": spike_metrics['domain'],
                    "leg": spike_metrics['leg'],
                    "service": spike_metrics.get('service', 'N/A'),
                    "max_metrics": spike_metrics.to_dict(),
                })
    return spikes

# Step 4: Correlate Anomalies
def correlate_anomalies(df):
    anomalies = []

    for i, row in df.iterrows():
        causes = []
        if row['messagedurations'] > MESSAGE_SPIKE_THRESHOLD:
            causes.append("High Response Time")
        if row['serverdurations'] >= 0.8 * row['messagedurations']:
            causes.append("Backend Issue")
        if row['requestmessagecount'] > TPS_THRESHOLD:
            causes.append("High TPS")
        if row['cpu'] > CPU_THRESHOLD:
            causes.append("High CPU")
        if row['memory'] > MEMORY_THRESHOLD:
            causes.append("High Memory")
        if row['worklist'] > WORKLIST_THRESHOLD:
            causes.append("High Worklist")
        if row['load'] > LOAD_THRESHOLD:
            causes.append("High Load")
        
        if causes:  # If any cause is identified
            anomalies.append({
                "timestamp": row['timestamp'],
                "dpbox": row['dpbox'],
                "domain": row['domain'],
                "leg": row['leg'],
                "service": row.get('service', 'N/A'),
                "causes": ", ".join(causes),
                "metrics": row.to_dict()  # Include all metrics
            })

    return anomalies

# Step 5: Write to Text Files
def write_to_txt(file_name, data):
    try:
        # Ensure the reports folder exists
        os.makedirs("reports", exist_ok=True)
        file_path = os.path.join("reports", file_name)

        # Write data to the file in JSON format
        with open(file_path, "w") as f:
            for entry in data:
                f.write(json.dumps(entry) + "\n")
        print(f"Data successfully written to {file_path}")
    except Exception as e:
        print(f"Error writing data to {file_name}: {e}")

# Step 6: Main Execution with Command-Line Input
if __name__ == "__main__":
    # Parse command-line arguments for start and end time
    parser = argparse.ArgumentParser(description="Spike and Anomaly Detection")
    parser.add_argument("--start_time", required=True, help="Start time in format YYYY-MM-DD HH:MM:SS")
    parser.add_argument("--end_time", required=True, help="End time in format YYYY-MM-DD HH:MM:SS")
    args = parser.parse_args()

    try:
        # Validate time inputs
        start_time = datetime.strptime(args.start_time, "%Y-%m-%d %H:%M:%S")
        end_time = datetime.strptime(args.end_time, "%Y-%m-%d %H:%M:%S")
    except ValueError as e:
        print(f"Invalid time format: {e}")
        exit(1)
    
    # Fetch data
    print(f"Fetching data from {start_time} to {end_time}...")
    durations_df, counts_df, metrics_df = fetch_data(start_time, end_time)
    
    if durations_df is not None and counts_df is not None and metrics_df is not None:
        # Merge data
        print("Merging data...")
        merged_data = merge_data(durations_df, counts_df, metrics_df)
        
        # Detect spikes
        print("Detecting spikes...")
        spikes = detect_spikes(merged_data)
        print(f"Detected {len(spikes)} spikes.")
        
        # Correlate anomalies
        print("Detecting anomalies...")
        anomalies = correlate_anomalies(merged_data)
        print(f"Detected {len(anomalies)} anomalies.")
        
        # Write spikes to text file
        print("Writing spikes to text file...")
        write_to_txt("spikes_report.txt", spikes)
        
        # Write anomalies to text file
        print("Writing anomalies to text file...")
        write_to_txt("anomalies_report.txt", anomalies)
    else:
        print("Failed to fetch data. Exiting.")
