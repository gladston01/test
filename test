import psycopg2
import pandas as pd
import json
import os
from datetime import datetime
import argparse

REPORTS_FOLDER = "reports"
HISTORIC_SLA_FILE = "historic_sla.json"

ANOMALY_THRESHOLDS = {
    "MESSAGE_SPIKE_THRESHOLD": 200,
    "TPS_THRESHOLD": 300,
    "CPU_THRESHOLD": 50,
    "MEMORY_THRESHOLD": 50,
    "WORKLIST_THRESHOLD": 800,
    "LOAD_THRESHOLD": 80,
}


def parse_time(input_time):
    """
    Parses input time string to datetime object.
    """
    try:
        return datetime.strptime(input_time, "%Y-%m-%d %H:%M:%S")
    except ValueError:
        print(f"Invalid time format: {input_time}")
        raise ValueError("Please provide a valid time in 'YYYY-MM-DD HH:MM:SS' format.")


def query_pgsql_with_multiple_metrics(start_time, end_time, db_config):
    """
    Fetches data from multiple tables and returns as DataFrames.
    """
    queries = {
        "durations": """
            SELECT dpbox, leg, domain, service, messagedurations, requestdurations, responsedurations, serverdurations, timestamp
            FROM dpmessagedurations
            WHERE timestamp BETWEEN %s AND %s
        """,
        "counts": """
            SELECT dpbox, leg, domain, service, errormessagecount, requestmessagecount, rresponsemessagecount AS responsemessagecount, timestamp
            FROM dpmessagecounts
            WHERE timestamp BETWEEN %s AND %s
        """,
        "metrics": """
            SELECT dpbox, leg, domain, cpu, worklist, memory, load, timestamp
            FROM dpsystemmetrics
            WHERE timestamp BETWEEN %s AND %s
        """
    }

    try:
        conn = psycopg2.connect(
            host=db_config['host'],
            dbname=db_config['dbname'],
            user=db_config['user'],
            password=db_config['password'],
            port=db_config['port']
        )
        
        dataframes = {}
        for key, query in queries.items():
            with conn.cursor() as cursor:
                cursor.execute(query, (start_time, end_time))
                columns = [desc[0] for desc in cursor.description]
                results = cursor.fetchall()
                dataframes[key] = pd.DataFrame(results, columns=columns)
                
    except Exception as e:
        print(f"Error querying the database: {e}")
        return None
    finally:
        if conn:
            conn.close()
    
    return dataframes


def detect_anomalies(durations_df, counts_df, metrics_df):
    """
    Detects anomalies across multiple metrics.
    """
    anomalies = {
        "high_response_time": [],
        "backend_issue": [],
        "high_tps": [],
        "high_cpu": [],
        "high_memory": [],
        "high_worklist": [],
        "high_load": [],
    }

    # Standardize column names
    durations_df.rename(columns=str.lower, inplace=True)
    counts_df.rename(columns=str.lower, inplace=True)
    metrics_df.rename(columns=str.lower, inplace=True)

    # High Response Time and Backend Issue Detection
    for idx, row in durations_df.iterrows():
        service, domain, leg, timestamp = row["service"], row["domain"], row["leg"], row["timestamp"]
        
        # High Response Time
        if row["messagedurations"] > ANOMALY_THRESHOLDS["MESSAGE_SPIKE_THRESHOLD"]:
            anomalies["high_response_time"].append(f"{service} in {domain} {leg} exceeded response time at {timestamp}")
        
        # Backend Issue
        if row["serverdurations"] >= 0.8 * row["messagedurations"] > ANOMALY_THRESHOLDS["MESSAGE_SPIKE_THRESHOLD"]:
            anomalies["backend_issue"].append(f"{service} in {domain} {leg} backend issue at {timestamp}")

    # High TPS Detection
    for idx, row in counts_df.iterrows():
        if row["requestmessagecount"] > ANOMALY_THRESHOLDS["TPS_THRESHOLD"]:
            anomalies["high_tps"].append(f"{row['service']} in {row['domain']} {row['leg']} exceeded TPS at {row['timestamp']}")

    # High System Metrics Detection
    for idx, row in metrics_df.iterrows():
        domain, leg, timestamp = row["domain"], row["leg"], row["timestamp"]
        if row["cpu"] > ANOMALY_THRESHOLDS["CPU_THRESHOLD"]:
            anomalies["high_cpu"].append(f"High CPU usage in {domain} {leg} at {timestamp}")
        if row["memory"] > ANOMALY_THRESHOLDS["MEMORY_THRESHOLD"]:
            anomalies["high_memory"].append(f"High memory usage in {domain} {leg} at {timestamp}")
        if row["worklist"] > ANOMALY_THRESHOLDS["WORKLIST_THRESHOLD"]:
            anomalies["high_worklist"].append(f"High worklist queue in {domain} {leg} at {timestamp}")
        if row["load"] > ANOMALY_THRESHOLDS["LOAD_THRESHOLD"]:
            anomalies["high_load"].append(f"High system load in {domain} {leg} at {timestamp}")
    
    return anomalies


def log_anomalies(anomalies):
    """
    Logs anomalies into text files.
    """
    os.makedirs(REPORTS_FOLDER, exist_ok=True)
    for anomaly_type, messages in anomalies.items():
        file_path = os.path.join(REPORTS_FOLDER, f"{anomaly_type}_anomalies.txt")
        with open(file_path, "w") as file:
            if messages:
                file.write("\n".join(messages))
            else:
                file.write(f"No {anomaly_type.replace('_', ' ')} anomalies detected.\n")
    print(f"Anomalies logged to {REPORTS_FOLDER}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Query PostgreSQL and detect SLA breaches in duration metrics.")
    parser.add_argument("--start-time", type=str, required=True, help="Start time in 'YYYY-MM-DD HH:MM:SS' format.")
    parser.add_argument("--end-time", type=str, required=True, help="End time in 'YYYY-MM-DD HH:MM:SS' format.")
    args = parser.parse_args()

    try:
        start_time = parse_time(args.start_time)
        end_time = parse_time(args.end_time)
    except ValueError as e:
        print(e)
        exit(1)

    db_config = {
        'host': 'localhost',
        'dbname': 'your_db_name',
        'user': 'your_user',
        'password': 'your_password',
        'port': 5432
    }

    # Fetch data
    dataframes = query_pgsql_with_multiple_metrics(start_time, end_time, db_config)
    
    if dataframes:
        anomalies = detect_anomalies(dataframes["durations"], dataframes["counts"], dataframes["metrics"])
        log_anomalies(anomalies)
        print("Anomaly detection complete.")
    else:
        print("No data found for the given time range.")
