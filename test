import psycopg2
import pandas as pd
import json
from datetime import datetime
import argparse
import os
import numpy as np

HISTORIC_SLA_FILE = "historic_sla.json"
REPORTS_FOLDER = "reports"


def parse_time(input_time):
    """
    Parses the input time and ensures it is in the correct format.
    """
    try:
        parsed_time = datetime.strptime(input_time, "%Y-%m-%d %H:%M:%S")
    except ValueError:
        try:
            parsed_time = datetime.fromisoformat(input_time)
        except ValueError:
            print(f"Invalid time format: {input_time}")
            raise ValueError("Please provide a valid time in 'YYYY-MM-DD HH:MM:SS' format.")
    return parsed_time.strftime("%Y-%m-%d %H:%M:%S")


def query_pgsql(start_time, end_time, db_config):
    """
    Queries the PostgreSQL database for services, domains, legs, and their duration metrics.
    """
    sql_query = """
    SELECT 
        service,
        domain,
        leg,
        "timestamp",
        messagedurations,
        requestdurations,
        responsedurations,
        serverdurations
    FROM dpmessagedurations
    WHERE 
        "timestamp" BETWEEN %s AND %s
    """

    try:
        conn = psycopg2.connect(
            host=db_config['host'],
            dbname=db_config['dbname'],
            user=db_config['user'],
            password=db_config['password'],
            port=db_config['port']
        )

        with conn.cursor() as cursor:
            cursor.execute(sql_query, (start_time, end_time))
            columns = [desc[0] for desc in cursor.description]
            results = cursor.fetchall()

        df = pd.DataFrame(results, columns=columns)
    except Exception as e:
        print(f"Error querying the database: {e}")
        return None
    finally:
        if conn:
            conn.close()

    return df


def load_historic_sla():
    """
    Loads historic SLA values from a JSON file.
    """
    if os.path.exists(HISTORIC_SLA_FILE):
        with open(HISTORIC_SLA_FILE, "r") as file:
            return json.load(file)
    return {}


def save_historic_sla(historic_sla):
    """
    Saves historic SLA values to a JSON file.
    """
    def convert_numpy(obj):
        if isinstance(obj, (np.int64, np.int32)):
            return int(obj)
        elif isinstance(obj, (np.float64, np.float32)):
            return float(obj)
        return obj

    with open(HISTORIC_SLA_FILE, "w") as file:
        json.dump(historic_sla, file, indent=4, default=convert_numpy)


def detect_spikes_and_log(df, historic_sla, output_report, hard_sla=1000, sensitivity_factor=2, num_runs=5):
    """
    Detects spikes where duration values exceed hard SLA, moving average, or standard deviation thresholds.
    Generates a detailed report and separate alerts files for each metric and domain.
    """
    # Ensure reports folder exists
    os.makedirs(REPORTS_FOLDER, exist_ok=True)

    # Melt the dataframe to bring durations into a single column
    melted_df = df.melt(
        id_vars=["service", "domain", "leg", "timestamp"],
        value_vars=["messagedurations", "requestdurations", "responsedurations", "serverdurations"],
        var_name="duration_type",
        value_name="value"
    )

    spike_report = []
    alert_reports = {}
    for service in melted_df["service"].unique():
        service_df = melted_df[melted_df["service"] == service]
        for duration_type in ["messagedurations", "requestdurations", "responsedurations", "serverdurations"]:
            duration_df = service_df[service_df["duration_type"] == duration_type]

            if service in historic_sla and duration_type in historic_sla[service]:
                historic_max_values = historic_sla[service][duration_type].get("max_values", [])
                if len(historic_max_values) > 0:
                    moving_avg = sum(historic_max_values) / len(historic_max_values)
                    std_dev = (
                        (sum((x - moving_avg) ** 2 for x in historic_max_values) / len(historic_max_values)) ** 0.5
                    )
                else:
                    moving_avg = 0
                    std_dev = 0
            else:
                moving_avg = 0
                std_dev = 0

            max_value = duration_df["value"].max()
            spike_detected = False
            spike_reason = []

            # Hard SLA check
            if max_value > hard_sla:
                spike_detected = True
                spike_reason.append(f"Exceeds Hard SLA of {hard_sla}ms")

            # Moving Average + Standard Deviation Check
            if max_value > moving_avg + sensitivity_factor * std_dev:
                spike_detected = True
                spike_reason.append(
                    f"Exceeds Moving Avg of {moving_avg:.2f}ms + {sensitivity_factor} * Std Dev: {std_dev:.2f}ms"
                )

            # Sudden Spike Detection
            sudden_spike_detected = False
            sudden_spike_start = None
            sudden_spike_end = None
            previous_value = None
            for idx, row in duration_df.iterrows():
                if previous_value is not None and abs(row["value"] - previous_value) > 400:  # Sudden increase
                    if not sudden_spike_detected:
                        sudden_spike_start = row["timestamp"]
                        sudden_spike_detected = True
                    sudden_spike_end = row["timestamp"]
                elif sudden_spike_detected:
                    break  # Spike ended
                previous_value = row["value"]

            if sudden_spike_detected:
                spike_detected = True
                if sudden_spike_start == sudden_spike_end:
                    spike_reason.append(f"Sudden Spike at {sudden_spike_start}, reaching {max_value}ms")
                else:
                    spike_reason.append(f"Spike between {sudden_spike_start} and {sudden_spike_end}, reaching {max_value}ms")

            # Log spike report
            if spike_detected:
                spike_report.append(
                    f"Service: {service}, Domain: {row['domain']}, Leg: {row['leg']}, Metric: {duration_type}, "
                    f"Max Value: {max_value}, Reasons: {', '.join(spike_reason)}"
                )

                # Metric and domain-specific alert entry
                domain = row["domain"]
                alert_key = (duration_type, domain)
                if alert_key not in alert_reports:
                    alert_reports[alert_key] = []
                alert_reports[alert_key].append(
                    f"Timestamp: {row['timestamp']}, Service: {service}, Domain: {domain}, Leg: {row['leg']}, "
                    f"Metric: {duration_type}, Max Value: {max_value}, Reasons: {', '.join(spike_reason)}"
                )

            # Update historic SLA
            if service not in historic_sla:
                historic_sla[service] = {}
            if duration_type not in historic_sla[service]:
                historic_sla[service][duration_type] = {"max_values": []}
            historic_sla[service][duration_type]["max_values"] = (
                historic_sla[service][duration_type]["max_values"][-(num_runs - 1):] + [max_value]
            )

    # Write detailed report
    output_report_file = os.path.join(REPORTS_FOLDER, output_report)
    with open(output_report_file, "w") as f:
        if spike_report:
            f.write("Detailed Report:\n")
            f.write("\n".join(spike_report))
            print(f"Detailed report logged to {output_report_file}")
        else:
            f.write("No SLA breaches detected.\n")
            print("No SLA breaches detected.")

    # Write separate alert files for each metric and domain
    for (metric, domain), alerts in alert_reports.items():
        metric_alert_file = os.path.join(REPORTS_FOLDER, f"DP_Alerts_{metric}_{domain}.txt")
        with open(metric_alert_file, "w") as f:
            if alerts:
                f.write("Alerts Detected:\n")
                f.write("\n".join(alerts))
                print(f"Alerts for {metric} in {domain} logged to {metric_alert_file}")
            else:
                f.write("No Alerts Detected.\n")
                print(f"No Alerts Detected for {metric} in {domain}.")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Query PostgreSQL and detect SLA breaches in duration metrics.")
    parser.add_argument("--start-time", type=str, required=True, help="Start time in 'YYYY-MM-DD HH:MM:SS' format.")
    parser.add_argument("--end-time", type=str, required=True, help="End time in 'YYYY-MM-DD HH:MM:SS' format.")
    args = parser.parse_args()

    try:
        start_time = parse_time(args.start_time)
        end_time = parse_time(args.end_time)
    except ValueError as e:
        print(e)
        exit(1)

    db_config = {
        'host': 'localhost',
        'dbname': 'your_db_name',
        'user': 'your_user',
        'password': 'your_password',
        'port': 5432
    }

    result_df = query_pgsql(start_time, end_time, db_config)

    if result_df is not None and not result_df.empty:
        historic_sla = load_historic_sla()
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        detect_spikes_and_log(result_df, historic_sla, f"DP_Analysis_{timestamp}.txt")
        save_historic_sla(historic_sla)
    else:
        print("No data found for the given time range.")
