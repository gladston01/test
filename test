import psycopg2
import pandas as pd
from datetime import datetime
import argparse


def parse_time(input_time):
    """
    Parses the input time and ensures it is in the correct format.
    
    Parameters:
        input_time (str): The input time string.
    
    Returns:
        str: The time string in 'YYYY-MM-DD HH:MM:SS' format.
    """
    try:
        # Try parsing the input time into a datetime object
        parsed_time = datetime.strptime(input_time, "%Y-%m-%d %H:%M:%S")
    except ValueError:
        try:
            # Attempt parsing with other common formats and reformat to desired format
            parsed_time = datetime.fromisoformat(input_time)
        except ValueError:
            print(f"Invalid time format: {input_time}")
            raise ValueError("Please provide a valid time in 'YYYY-MM-DD HH:MM:SS' format.")
    
    # Return the properly formatted time string
    return parsed_time.strftime("%Y-%m-%d %H:%M:%S")


def query_pgsql(start_time, end_time, db_config):
    """
    Queries the PostgreSQL database for services and their duration metrics.

    Parameters:
        start_time (str): Start time in 'YYYY-MM-DD HH:MM:SS' format.
        end_time (str): End time in 'YYYY-MM-DD HH:MM:SS' format.
        db_config (dict): Database configuration with keys: host, dbname, user, password, port.

    Returns:
        pd.DataFrame: DataFrame containing the services, timestamp, and duration values.
    """
    # SQL Query to fetch the data
    sql_query = """
    SELECT 
        service,
        "timestamp",
        messagedurations,
        requestdurations,
        responsedurations,
        serverdurations
    FROM dpmessagedurations
    WHERE 
        "timestamp" BETWEEN %s AND %s
    """

    # Connect to the database
    try:
        conn = psycopg2.connect(
            host=db_config['host'],
            dbname=db_config['dbname'],
            user=db_config['user'],
            password=db_config['password'],
            port=db_config['port']
        )

        # Execute the query
        with conn.cursor() as cursor:
            cursor.execute(sql_query, (start_time, end_time))
            # Fetch the results
            columns = [desc[0] for desc in cursor.description]
            results = cursor.fetchall()

        # Convert results to a DataFrame
        df = pd.DataFrame(results, columns=columns)

    except Exception as e:
        print(f"Error querying the database: {e}")
        return None
    finally:
        if conn:
            conn.close()

    return df


def detect_spikes_and_log(df, output_file):
    """
    Detects spikes where duration values are consistently over 1000ms and logs them to a TXT file.

    Parameters:
        df (pd.DataFrame): The DataFrame containing the query results.
        output_file (str): The path to the output TXT file.
    """
    # Melt the dataframe to bring durations into a single column
    melted_df = df.melt(
        id_vars=["service", "timestamp"],
        value_vars=["messagedurations", "requestdurations", "responsedurations", "serverdurations"],
        var_name="duration_type",
        value_name="value"
    )

    # Filter rows where value > 1000ms
    filtered_df = melted_df[melted_df["value"] > 1000]

    # Detect spikes
    spike_report = []
    for service in filtered_df["service"].unique():
        service_df = filtered_df[filtered_df["service"] == service].sort_values(by="timestamp")
        duration_type_groups = service_df.groupby("duration_type")

        for duration_type, group in duration_type_groups:
            start_time = None
            end_time = None
            spike_active = False
            max_value = 0
            count_over_1000 = 0

            for _, row in group.iterrows():
                if not spike_active:
                    # Start of a new spike
                    start_time = row["timestamp"]
                    spike_active = True

                # Update the end time
                end_time = row["timestamp"]

                # Track the max value and count of values > 1000
                max_value = max(max_value, row["value"])
                count_over_1000 += 1

            if spike_active:
                spike_report.append(
                    f"Service: {service}, Metric: {duration_type}, "
                    f"Spike from {start_time} to {end_time}, "
                    f"Max Value: {max_value}, Responses > 1000ms: {count_over_1000}"
                )

    # Write to TXT file
    with open(output_file, "w") as f:
        if spike_report:
            f.write("Alerts Detected:\n")
            f.write("\n".join(spike_report))
            print(f"Spikes logged to {output_file}")
        else:
            f.write("No spikes detected.\n")
            print("No spikes detected.")


if __name__ == "__main__":
    # Parse command-line arguments
    parser = argparse.ArgumentParser(description="Query PostgreSQL and detect spikes in duration metrics.")
    parser.add_argument("--start-time", type=str, required=True, help="Start time in 'YYYY-MM-DD HH:MM:SS' format.")
    parser.add_argument("--end-time", type=str, required=True, help="End time in 'YYYY-MM-DD HH:MM:SS' format.")
    args = parser.parse_args()

    try:
        # Parse and validate start and end times
        start_time = parse_time(args.start_time)
        end_time = parse_time(args.end_time)
    except ValueError as e:
        print(e)
        exit(1)

    # Database configuration
    db_config = {
        'host': 'localhost',       # Replace with your host
        'dbname': 'your_db_name',  # Replace with your database name
        'user': 'your_user',       # Replace with your username
        'password': 'your_password', # Replace with your password
        'port': 5432               # Default PostgreSQL port
    }

    # Query the database
    result_df = query_pgsql(start_time, end_time, db_config)

    if result_df is not None and not result_df.empty:
        # Output file
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"alerts_{timestamp}.txt"

        # Detect spikes and log to TXT
        detect_spikes_and_log(result_df, output_file)
    else:
        print("No data found for the given time range.")
