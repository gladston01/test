import pandas as pd
import os
import json
import argparse
from datetime import datetime
import psycopg2

# PostgreSQL Database Configuration
DB_CONFIG = {
    "dbname": "performance_db",
    "user": "username",
    "password": "password",
    "host": "localhost",
    "port": 5432,
}

# Thresholds for detecting spikes and anomalies
MESSAGE_SPIKE_THRESHOLD = 200  # Duration in ms to classify a spike
CPU_THRESHOLD = 50  # Percentage
MEMORY_THRESHOLD = 50  # Percentage
TPS_THRESHOLD = 300  # Transactions per second
WORKLIST_THRESHOLD = 800
LOAD_THRESHOLD = 80

# Step 1: Fetch Data from PostgreSQL
def fetch_data(start_time, end_time):
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        
        durations_query = f"""
        SELECT dpbox, leg, domain, service, messagedurations, requestdurations, responsedurations, serverdurations, timestamp
        FROM dpmessagedurations
        WHERE timestamp BETWEEN '{start_time}' AND '{end_time}';
        """
        counts_query = f"""
        SELECT dpbox, leg, domain, service, errormessagecount, requestmessagecount, rresponsemessagecount, timestamp
        FROM dpmessagecounts
        WHERE timestamp BETWEEN '{start_time}' AND '{end_time}';
        """
        metrics_query = f"""
        SELECT dpbox, leg, domain, cpu, worklist, memory, load, timestamp
        FROM dpsystemmetrics
        WHERE timestamp BETWEEN '{start_time}' AND '{end_time}';
        """
        
        durations_df = pd.read_sql_query(durations_query, conn)
        counts_df = pd.read_sql_query(counts_query, conn)
        metrics_df = pd.read_sql_query(metrics_query, conn)
        
        conn.close()
        return durations_df, counts_df, metrics_df
    except Exception as e:
        print(f"Error fetching data: {e}")
        return None, None, None

# Step 2: Merge Data
def merge_data(durations_df, counts_df, metrics_df):
    merged_df = pd.merge(
        durations_df, counts_df, 
        on=['dpbox', 'leg', 'domain', 'service', 'timestamp'], 
        how='inner'
    )
    merged_df = pd.merge(
        merged_df, metrics_df, 
        on=['dpbox', 'leg', 'domain', 'timestamp'], 
        how='inner'
    )
    return merged_df.fillna(0)

# Step 3: Detect Spikes
def detect_spikes(df):
    spikes = []
    spike_active = False
    spike_start = None
    spike_metrics = None

    for i, row in df.iterrows():
        if row['messagedurations'] > MESSAGE_SPIKE_THRESHOLD:
            if not spike_active:
                spike_active = True
                spike_start = row['timestamp']
                spike_metrics = row
            else:
                spike_metrics = row.combine(spike_metrics, max)
        else:
            if spike_active:
                spike_active = False
                spikes.append({
                    "start_time": spike_start,
                    "end_time": row['timestamp'],
                    "dpbox": spike_metrics['dpbox'],
                    "domain": spike_metrics['domain'],
                    "leg": spike_metrics['leg'],
                    "service": spike_metrics.get('service', 'N/A'),
                    "max_metrics": spike_metrics.to_dict(),
                })
    return spikes

# Step 4: Correlate Anomalies
def correlate_anomalies(df):
    anomalies = []
    for _, row in df.iterrows():
        causes = []
        if row['messagedurations'] > MESSAGE_SPIKE_THRESHOLD: causes.append("High Response Time")
        if row['serverdurations'] >= 0.8 * row['messagedurations']: causes.append("Backend Issue")
        if row['requestmessagecount'] > TPS_THRESHOLD: causes.append("High TPS")
        if row['cpu'] > CPU_THRESHOLD: causes.append("High CPU")
        if row['memory'] > MEMORY_THRESHOLD: causes.append("High Memory")
        if row['worklist'] > WORKLIST_THRESHOLD: causes.append("High Worklist")
        if row['load'] > LOAD_THRESHOLD: causes.append("High Load")
        
        if causes:
            anomalies.append({
                "timestamp": row['timestamp'],
                "dpbox": row['dpbox'],
                "domain": row['domain'],
                "leg": row['leg'],
                "service": row.get('service', 'N/A'),
                "causes": ", ".join(causes),
                "metrics": row.to_dict(),
            })
    return anomalies

# Step 5: Convert Timestamps and Write to Text Files
def convert_timestamps(obj):
    if isinstance(obj, pd.Timestamp):
        return obj.isoformat()
    elif isinstance(obj, dict):
        return {k: convert_timestamps(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_timestamps(item) for item in obj]
    else:
        return obj

def write_to_txt(file_name, data):
    try:
        os.makedirs("reports", exist_ok=True)
        file_path = os.path.join("reports", file_name)
        with open(file_path, "w") as f:
            for entry in data:
                serialized_entry = convert_timestamps(entry)
                f.write(json.dumps(serialized_entry) + "\n")
        print(f"Data successfully written to {file_path}")
    except Exception as e:
        print(f"Error writing data to {file_name}: {e}")

# Main Execution
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--start_time", required=True, help="Start time: YYYY-MM-DD HH:MM:SS")
    parser.add_argument("--end_time", required=True, help="End time: YYYY-MM-DD HH:MM:SS")
    args = parser.parse_args()

    durations_df, counts_df, metrics_df = fetch_data(args.start_time, args.end_time)
    if durations_df is not None and counts_df is not None and metrics_df is not None:
        merged_data = merge_data(durations_df, counts_df, metrics_df)

        print("Detecting spikes...")
        spikes = detect_spikes(merged_data)
        print(f"Detected {len(spikes)} spikes.")
        write_to_txt("spikes_report.txt", spikes)

        print("Detecting anomalies...")
        anomalies = correlate_anomalies(merged_data)
        print(f"Detected {len(anomalies)} anomalies.")
        write_to_txt("anomalies_report.txt", anomalies)
    else:
        print("Failed to fetch data. Exiting.")
