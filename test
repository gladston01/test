import psycopg2
import pandas as pd
import os
import json
from datetime import datetime
import argparse

# Configuration
REPORTS_FOLDER = "reports"
HISTORIC_SLA_FILE = "historic_sla.json"

ANOMALY_THRESHOLDS = {
    "MESSAGE_SPIKE_THRESHOLD": 200,
    "TPS_THRESHOLD": 300,
    "CPU_THRESHOLD": 50,
    "MEMORY_THRESHOLD": 50,
    "WORKLIST_THRESHOLD": 800,
    "LOAD_THRESHOLD": 80,
}

# Exclusion lists for specific checks
EXCLUDE_SERVICES_TPS = ["ALL"]
EXCLUDE_SERVICES_MESSAGESPIKE = ["ALL"]


def parse_time(input_time):
    try:
        return datetime.strptime(input_time, "%Y-%m-%d %H:%M:%S")
    except ValueError:
        raise ValueError("Please provide a valid time in 'YYYY-MM-DD HH:MM:SS' format.")


def query_pgsql_with_multiple_metrics(start_time, end_time, db_config):
    """
    Fetches data from multiple tables and returns as DataFrames.
    """
    queries = {
        "durations": """
            SELECT dpbox, leg, domain, service, messagedurations, requestdurations, responsedurations, serverdurations, timestamp
            FROM dpmessagedurations
            WHERE timestamp BETWEEN %s AND %s
        """,
        "counts": """
            SELECT dpbox, leg, domain, service, errormessagecount, requestmessagecount, rresponsemessagecount AS responsemessagecount, timestamp
            FROM dpmessagecounts
            WHERE timestamp BETWEEN %s AND %s
        """,
        "metrics": """
            SELECT dpbox, leg, domain, cpu, worklist, memory, load, timestamp
            FROM dpsystemmetrics
            WHERE timestamp BETWEEN %s AND %s
        """
    }
    try:
        conn = psycopg2.connect(**db_config)
        dataframes = {}
        for key, query in queries.items():
            with conn.cursor() as cursor:
                cursor.execute(query, (start_time, end_time))
                columns = [desc[0] for desc in cursor.description]
                results = cursor.fetchall()
                dataframes[key] = pd.DataFrame(results, columns=columns)
                print(f"Fetched {len(results)} rows for {key} data.")
    except Exception as e:
        print(f"Error querying the database: {e}")
        return None
    finally:
        if conn:
            conn.close()

    return dataframes


def detect_spike_times(df, column, threshold):
    """
    Detects start and end times of spikes exceeding the threshold.
    """
    spikes = []
    current_spike = {"start": None, "end": None, "max_value": None}

    for _, row in df.iterrows():
        value, timestamp = row[column], row["timestamp"]

        if value > threshold:
            if current_spike["start"] is None:
                current_spike["start"] = timestamp
                current_spike["max_value"] = value
            current_spike["end"] = timestamp
            current_spike["max_value"] = max(current_spike["max_value"], value)
        else:
            if current_spike["start"]:
                spikes.append(
                    f"Spike detected: Start={current_spike['start']}, End={current_spike['end']}, Max Value={current_spike['max_value']} ms."
                )
                current_spike = {"start": None, "end": None, "max_value": None}

    if current_spike["start"]:
        spikes.append(
            f"Spike detected: Start={current_spike['start']}, End={current_spike['end']}, Max Value={current_spike['max_value']} ms."
        )

    return spikes


def detect_anomalies(durations_df, counts_df, metrics_df):
    """
    Detects anomalies across message spikes, TPS, and system metrics.
    """
    anomalies = {"message_spikes": [], "tps_anomalies": [], "system_metric_anomalies": [], "backend_issues": []}

    # Filter out excluded services
    durations_df = durations_df[~durations_df["service"].isin(EXCLUDE_SERVICES_MESSAGESPIKE)]
    counts_df = counts_df[~counts_df["service"].isin(EXCLUDE_SERVICES_TPS)]

    # Message Spike Detection
    spikes = detect_spike_times(durations_df, "messagedurations", ANOMALY_THRESHOLDS["MESSAGE_SPIKE_THRESHOLD"])
    anomalies["message_spikes"].extend(spikes)

    # TPS Anomalies
    for _, row in counts_df.iterrows():
        if row["requestmessagecount"] > ANOMALY_THRESHOLDS["TPS_THRESHOLD"]:
            anomalies["tps_anomalies"].append(
                f"At {row['timestamp']}, {row['service']} exceeded TPS threshold with {row['requestmessagecount']} requests."
            )

    # System Metrics Anomalies
    for _, row in metrics_df.iterrows():
        if row["cpu"] > ANOMALY_THRESHOLDS["CPU_THRESHOLD"]:
            anomalies["system_metric_anomalies"].append(
                f"At {row['timestamp']}, high CPU usage detected in {row['domain']} {row['leg']}, reaching {row['cpu']}%."
            )
        if row["memory"] > ANOMALY_THRESHOLDS["MEMORY_THRESHOLD"]:
            anomalies["system_metric_anomalies"].append(
                f"At {row['timestamp']}, high memory usage detected in {row['domain']} {row['leg']}, reaching {row['memory']}%."
            )

    # Backend Issues
    for _, row in durations_df.iterrows():
        if row["serverdurations"] >= 0.8 * row["messagedurations"] > ANOMALY_THRESHOLDS["MESSAGE_SPIKE_THRESHOLD"]:
            anomalies["backend_issues"].append(
                f"At {row['timestamp']}, backend issue detected. Message Duration: {row['messagedurations']} ms, "
                f"Server Duration: {row['serverdurations']} ms."
            )

    return anomalies


def log_anomalies(anomalies):
    os.makedirs(REPORTS_FOLDER, exist_ok=True)
    for anomaly_type, messages in anomalies.items():
        file_path = os.path.join(REPORTS_FOLDER, f"{anomaly_type}_anomalies.txt")
        with open(file_path, "w") as file:
            if messages:
                file.write("\n".join(messages))
            else:
                file.write(f"No {anomaly_type.replace('_', ' ')} anomalies detected.\n")
    print(f"Anomalies logged to {REPORTS_FOLDER}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Detect anomalies with spike detection and backend checks.")
    parser.add_argument("--start-time", type=str, required=True, help="Start time in 'YYYY-MM-DD HH:MM:SS'.")
    parser.add_argument("--end-time", type=str, required=True, help="End time in 'YYYY-MM-DD HH:MM:SS'.")
    args = parser.parse_args()

    db_config = {
        'host': 'localhost',
        'dbname': 'your_db_name',
        'user': 'your_user',
        'password': 'your_password',
        'port': 5432
    }

    try:
        start_time = parse_time(args.start_time)
        end_time = parse_time(args.end_time)
    except ValueError as e:
        print(e)
        exit(1)

    dataframes = query_pgsql_with_multiple_metrics(start_time, end_time, db_config)

    if dataframes:
        anomalies = detect_anomalies(dataframes["durations"], dataframes["counts"], dataframes["metrics"])
        log_anomalies(anomalies)
        print("Anomaly detection complete.")
    else:
        print("No data found for the given time range.")
