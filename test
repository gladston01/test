import psycopg2
import pandas as pd
import json
from datetime import datetime
import argparse
import os
import numpy as np

HISTORIC_SLA_FILE = "historic_sla.json"
REPORTS_FOLDER = "reports"


def parse_time(input_time):
    """
    Parses the input time and ensures it is in the correct format.
    """
    try:
        parsed_time = datetime.strptime(input_time, "%Y-%m-%d %H:%M:%S")
    except ValueError:
        try:
            parsed_time = datetime.fromisoformat(input_time)
        except ValueError:
            print(f"Invalid time format: {input_time}")
            raise ValueError("Please provide a valid time in 'YYYY-MM-DD HH:MM:SS' format.")
    return parsed_time.strftime("%Y-%m-%d %H:%M:%S")


def query_pgsql(start_time, end_time, db_config):
    """
    Queries the PostgreSQL database for services, domains, legs, and their duration metrics.
    """
    sql_query = """
    SELECT 
        service,
        domain,
        leg,
        "timestamp",
        messagedurations,
        requestdurations,
        responsedurations,
        serverdurations
    FROM dpmessagedurations
    WHERE 
        "timestamp" BETWEEN %s AND %s
    """

    try:
        conn = psycopg2.connect(
            host=db_config['host'],
            dbname=db_config['dbname'],
            user=db_config['user'],
            password=db_config['password'],
            port=db_config['port']
        )

        with conn.cursor() as cursor:
            cursor.execute(sql_query, (start_time, end_time))
            columns = [desc[0] for desc in cursor.description]
            results = cursor.fetchall()

        df = pd.DataFrame(results, columns=columns)
    except Exception as e:
        print(f"Error querying the database: {e}")
        return None
    finally:
        if conn:
            conn.close()

    return df


def load_historic_sla():
    """
    Loads historic SLA values from a JSON file.
    """
    if os.path.exists(HISTORIC_SLA_FILE):
        with open(HISTORIC_SLA_FILE, "r") as file:
            return json.load(file)
    return {}


def save_historic_sla(historic_sla):
    """
    Saves historic SLA values to a JSON file.
    """
    def convert_numpy(obj):
        if isinstance(obj, (np.int64, np.int32)):
            return int(obj)
        elif isinstance(obj, (np.float64, np.float32)):
            return float(obj)
        return obj

    with open(HISTORIC_SLA_FILE, "w") as file:
        json.dump(historic_sla, file, indent=4, default=convert_numpy)


def detect_spikes(duration_df, threshold):
    """
    Detects distinct spikes in the data.

    Args:
        duration_df (pd.DataFrame): DataFrame with timestamps and values.
        threshold (float): The threshold for identifying spikes.

    Returns:
        List[Tuple]: A list of spikes with their start time, end time, and max value.
    """
    spikes = []
    current_spike = {"start_time": None, "end_time": None, "max_value": None}

    for idx, row in duration_df.iterrows():
        value = row["value"]
        timestamp = row["timestamp"]

        # Check if the value exceeds the threshold
        if value > threshold:
            if current_spike["start_time"] is None:
                # Start a new spike
                current_spike["start_time"] = timestamp
                current_spike["max_value"] = value
            else:
                # Update the current spike
                current_spike["max_value"] = max(current_spike["max_value"], value)
            # Always update the end time of the spike
            current_spike["end_time"] = timestamp
        else:
            # If we encounter a normal value, finalize the current spike
            if current_spike["start_time"] is not None:
                spikes.append(
                    (current_spike["start_time"], current_spike["end_time"], current_spike["max_value"])
                )
                # Reset the current spike
                current_spike = {"start_time": None, "end_time": None, "max_value": None}

    # Finalize the last spike if still active
    if current_spike["start_time"] is not None:
        spikes.append(
            (current_spike["start_time"], current_spike["end_time"], current_spike["max_value"])
        )

    return spikes


def detect_spikes_and_log(df, historic_sla, moving_sla_files, hard_sla=1000, sensitivity_factor=2, num_runs=5):
    """
    Detects spikes where duration values exceed hard SLA, moving average, or standard deviation thresholds.
    Generates separate alert files for each domain and metric, and single Moving SLA files for each metric.
    """
    # Ensure reports folder exists
    os.makedirs(REPORTS_FOLDER, exist_ok=True)

    # Melt the dataframe to bring durations into a single column
    melted_df = df.melt(
        id_vars=["service", "domain", "leg", "timestamp"],
        value_vars=["messagedurations", "requestdurations", "responsedurations", "serverdurations"],
        var_name="duration_type",
        value_name="value"
    )

    moving_sla_breaches = {metric: [] for metric in ["messagedurations", "requestdurations", "responsedurations", "serverdurations"]}
    domain_alerts = {}

    for service in melted_df["service"].unique():
        service_df = melted_df[melted_df["service"] == service]
        for duration_type in ["messagedurations", "requestdurations", "responsedurations", "serverdurations"]:
            duration_df = service_df[service_df["duration_type"] == duration_type]

            if service in historic_sla and duration_type in historic_sla[service]:
                historic_max_values = historic_sla[service][duration_type].get("max_values", [])
                if len(historic_max_values) > 0:
                    moving_avg = sum(historic_max_values) / len(historic_max_values)
                    std_dev = (
                        (sum((x - moving_avg) ** 2 for x in historic_max_values) / len(historic_max_values)) ** 0.5
                    )
                else:
                    moving_avg = 0
                    std_dev = 0
            else:
                moving_avg = 0
                std_dev = 0

            max_value = duration_df["value"].max()
            domain = duration_df.iloc[-1]["domain"]
            leg = duration_df.iloc[-1]["leg"]
            timestamp = duration_df.iloc[-1]["timestamp"]

            # Use a tuple (domain, metric) as the key
            alert_key = (domain, duration_type)

            # Hard SLA Check
            if max_value > hard_sla:
                if alert_key not in domain_alerts:
                    domain_alerts[alert_key] = []
                domain_alerts[alert_key].append(
                    f"At {timestamp} for {service} in {domain} {leg} {duration_type} SLA exceeded with max value {max_value}"
                )

            # Moving SLA Check
            if max_value > moving_avg + sensitivity_factor * std_dev:
                moving_sla_breaches[duration_type].append(
                    f"At {timestamp} for {service} in {domain} {leg} {duration_type} exceeded Moving SLA of {moving_avg:.2f}ms with max value {max_value}"
                )

            # Sudden Spike Detection
            spike_threshold = max(moving_avg + 400, hard_sla)
            spikes = detect_spikes(duration_df, spike_threshold)

            for spike in spikes:
                start_time, end_time, spike_max = spike
                if alert_key not in domain_alerts:
                    domain_alerts[alert_key] = []
                if start_time == end_time:
                    domain_alerts[alert_key].append(
                        f"Sudden Spike detected for {service} in {domain} {leg} {duration_type} at {start_time}, reaching {spike_max}ms"
                    )
                else:
                    domain_alerts[alert_key].append(
                        f"Spike detected for {service} in {domain} {leg} {duration_type} between {start_time} and {end_time}, reaching {spike_max}ms"
                    )

            # Update Historic SLA
            if service not in historic_sla:
                historic_sla[service] = {}
            if duration_type not in historic_sla[service]:
                historic_sla[service][duration_type] = {"max_values": []}
            historic_sla[service][duration_type]["max_values"] = (
                historic_sla[service][duration_type]["max_values"][-(num_runs - 1):] + [max_value]
            )

    # Write Alerts to Files by Domain and Metric
    for (domain, metric), alerts in domain_alerts.items():
        alert_file = os.path.join(REPORTS_FOLDER, f"Alerts_{domain}_{metric}.txt")
        with open(alert_file, "w") as f:
            if alerts:
                f.write("Alerts Detected:\n")
                f.write("\n".join(alerts))
                print(f"Alerts for {domain} and {metric} logged to {alert_file}")
            else:
                f.write("No Alerts Detected.\n")
                print(f"No Alerts Detected for {domain} and {metric}.")

    # Write Moving SLA Breaches to Separate Files
    for metric, breaches in moving_sla_breaches.items():
        metric_file = os.path.join(REPORTS_FOLDER, moving_sla_files[metric])
        with open(metric_file, "w") as f:
            if breaches:
                f.write("Moving SLA Breaches Detected:\n")
                f.write("\n".join(breaches))
                print(f"Moving SLA breaches logged to {metric_file}")
            else:
                f.write("No Moving SLA breaches detected.\n")
                print(f"No Moving SLA breaches detected for {metric}.")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Query PostgreSQL and detect SLA breaches in duration metrics.")
    parser.add_argument("--start-time", type=str, required=True, help="Start time in 'YYYY-MM-DD HH:MM:SS' format.")
    parser.add_argument("--end-time", type=str, required=True, help="End time in 'YYYY-MM-DD HH:MM:SS' format.")
    args = parser.parse_args()

    try:
        start_time = parse_time(args.start_time)
        end_time = parse_time(args.end_time)
    except ValueError as e:
        print(e)
        exit(1)

    db_config = {
        'host': 'localhost',
        'dbname': 'your_db_name',
        'user': 'your_user',
        'password': 'your_password',
        'port': 5432
    }

    result_df = query_pgsql(start_time, end_time, db_config)

    if result_df is not None and not result_df.empty:
        historic_sla = load_historic_sla()
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        moving_sla_files = {
            "messagedurations": f"Moving_SLA_messagedurations_{timestamp}.txt",
            "requestdurations": f"Moving_SLA_requestdurations_{timestamp}.txt",
            "responsedurations": f"Moving_SLA_responsedurations_{timestamp}.txt",
            "serverdurations": f"Moving_SLA_serverdurations_{timestamp}.txt",
        }
        detect_spikes_and_log(result_df, historic_sla, moving_sla_files)
        save_historic_sla(historic_sla)
    else:
        print("No data found for the given time range.")
