import pandas as pd
import numpy as np
import json
import os
import logging
import argparse
import random
from datetime import datetime

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')

BASELINE_FILE = "baseline_stats.json"

def load_jtl(filepath):
    if not os.path.exists(filepath):
        logging.error(f"JTL file not found: {filepath}")
        return None
    col_names = [
        "timeStamp","elapsed","label","responseCode","responseMessage",
        "threadName","dataType","success","failureMessage","bytes","sentBytes",
        "grpThreads","allThreads","URL","Latency","IdleTime","Connect"
    ]
    try:
        df = pd.read_csv(filepath, names=col_names, header=0, dtype={'responseCode': 'object'}, low_memory=True)
        df['success'] = df['success'].apply(lambda x: str(x).lower() == 'true')
        df['timeStamp'] = pd.to_datetime(df['timeStamp'], unit='ms', errors='coerce')
        df.dropna(subset=['timeStamp','elapsed','success'], inplace=True)
        return df
    except Exception as e:
        logging.exception("Failed to read or process JTL file.")
        return None

def load_baseline():
    if os.path.exists(BASELINE_FILE):
        with open(BASELINE_FILE, 'r') as f:
            return json.load(f)
    else:
        return {}

def save_baseline(data):
    with open(BASELINE_FILE, 'w') as f:
        json.dump(data, f, indent=2)

def compute_incremental_stats(df):
    result = {}
    for label, group in df.groupby('label'):
        arr = group['elapsed'].values
        count = len(arr)
        sum_elapsed = arr.sum()
        sum_squares = (arr**2).sum()
        errors = (~group['success']).sum()

        result[label] = {
            'count': count,
            'sum': float(sum_elapsed),
            'sum_of_squares': float(sum_squares),
            'error_count': int(errors),
            'sample': arr.tolist()
        }
    return result

def update_baseline_with_run(baseline_data, run_stats, sample_size_per_label):
    for label, stats in run_stats.items():
        if label not in baseline_data:
            baseline_data[label] = {
                'count': 0, 'sum': 0.0, 'sum_of_squares': 0.0, 'error_count': 0,
                'sample': []
            }

        # Update aggregates
        baseline_data[label]['count'] += stats['count']
        baseline_data[label]['sum'] += stats['sum']
        baseline_data[label]['sum_of_squares'] += stats['sum_of_squares']
        baseline_data[label]['error_count'] += stats['error_count']

        # Update sample
        combined_sample = baseline_data[label]['sample'] + stats['sample']
        if len(combined_sample) > sample_size_per_label:
            combined_sample = random.sample(combined_sample, sample_size_per_label)
        baseline_data[label]['sample'] = combined_sample

        baseline_data[label]['last_updated'] = datetime.utcnow().isoformat() + "Z"
    return baseline_data

def compute_percentile_from_sample(sample, percentile=95):
    if not sample:
        return None
    return float(np.percentile(sample, percentile))

def analyze_current_run(current_stats, baseline_data, hard_avg_sla, hard_p95_sla, hard_error_sla, threshold_factor):
    alerts = []
    current_run_summary = []

    for label, stats in current_stats.items():
        curr_count = stats['count']
        if curr_count == 0:
            continue

        curr_avg = stats['sum'] / curr_count
        curr_err_rate = stats['error_count'] / curr_count
        curr_p95 = compute_percentile_from_sample(stats['sample'], 95)
        if curr_p95 is None:
            curr_p95 = curr_avg  # fallback

        sla_violations = []
        # Hard SLA checks
        if curr_avg > hard_avg_sla:
            sla_violations.append(f"Avg SLA breach: {curr_avg:.2f}ms > {hard_avg_sla}ms")
        if curr_p95 > hard_p95_sla:
            sla_violations.append(f"95th percentile SLA breach: {curr_p95:.2f}ms > {hard_p95_sla}ms")
        if curr_err_rate > hard_error_sla:
            sla_violations.append(f"Error rate SLA breach: {curr_err_rate*100:.2f}% > {hard_error_sla*100}%")

        # Historical comparison
        hist_mean = None
        if label in baseline_data and baseline_data[label]['count'] > 0:
            b = baseline_data[label]
            hist_mean = b['sum'] / b['count']
            if curr_avg > hist_mean * threshold_factor and curr_avg < hard_avg_sla:
                sla_violations.append(
                    f"{label}: Current avg {curr_avg:.2f}ms significantly worse than historical avg {hist_mean:.2f}ms"
                )

        current_run_summary.append({
            'label': label,
            'current_avg': curr_avg,
            'current_95p': curr_p95,
            'current_error_%': curr_err_rate*100,
            'historical_avg': hist_mean if hist_mean is not None else "N/A",
            'violations': "; ".join(sla_violations) if sla_violations else "None"
        })

        if sla_violations:
            alerts.append((label, sla_violations))

    return alerts, pd.DataFrame(current_run_summary)

def apply_data_aging(baseline_data, alpha=1.0):
    if alpha >= 1.0:
        return baseline_data
    for label, b in baseline_data.items():
        b['count'] = int(b['count'] * alpha)
        b['sum'] = b['sum'] * alpha
        b['sum_of_squares'] = b['sum_of_squares'] * alpha
        b['error_count'] = int(b['error_count'] * alpha)
        # sample remains as is or could be thinned
    return baseline_data

def export_report(df_summary, output_format, report_dir):
    # Ensure report_dir exists
    os.makedirs(report_dir, exist_ok=True)

    timestamp = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
    if output_format == 'csv':
        outfile = os.path.join(report_dir, f"report_{timestamp}.csv")
        df_summary.to_csv(outfile, index=False)
        logging.info(f"CSV report generated at: {outfile}")
    elif output_format == 'plain':
        outfile = os.path.join(report_dir, f"report_{timestamp}.txt")
        with open(outfile, 'w') as f:
            f.write("Current Run Summary:\n")
            f.write(df_summary.to_string(index=False))
        logging.info(f"Plain text report generated at: {outfile}")
    else:
        # Default: print on console (Should not happen if we validate output_format)
        logging.info("Current Run Summary:")
        logging.info(df_summary.to_string(index=False))

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Analyze JTL Performance with Historical Baselines and Reporting")

    parser.add_argument("--jtl-file", type=str, required=True, help="Path to the new run's JTL file")
    parser.add_argument("--hard-avg-sla", type=int, default=1000, help="Hard SLA for average (ms)")
    parser.add_argument("--hard-p95-sla", type=int, default=3000, help="Hard SLA for 95th percentile (ms)")
    parser.add_argument("--hard-error-sla", type=float, default=0.02, help="Hard SLA for error rate (fraction)")
    parser.add_argument("--threshold-factor", type=float, default=1.5, help="Factor for detecting regressions from historical average")
    parser.add_argument("--sample-size-per-label", type=int, default=1000, help="Size of rolling sample per label")
    parser.add_argument("--aging-alpha", type=float, default=1.0, help="Data aging factor (0 <= alpha <= 1)")
    parser.add_argument("--output-format", type=str, choices=['csv', 'plain'], default='plain', help="Output format for report")
    parser.add_argument("--report-dir", type=str, default="reports", help="Directory to store reports")

    args = parser.parse_args()

    # Argument Validation
    if not os.path.exists(args.jtl_file):
        logging.error("The specified JTL file does not exist.")
        exit(1)
    if args.hard_avg_sla <= 0 or args.hard_p95_sla <= 0 or args.hard_error_sla < 0:
        logging.error("SLA thresholds must be positive and error SLA must be >= 0.")
        exit(1)
    if not (0 <= args.aging_alpha <= 1):
        logging.error("aging-alpha must be between 0 and 1.")
        exit(1)
    if args.threshold_factor <= 0:
        logging.error("threshold-factor must be positive.")
        exit(1)

    # Ensure report directory exists upfront
    os.makedirs(args.report_dir, exist_ok=True)

    baseline_data = load_baseline()

    current_df = load_jtl(args.jtl_file)
    if current_df is None or current_df.empty:
        logging.error("No data to process in current run. Exiting.")
        exit(1)

    current_stats = compute_incremental_stats(current_df)
    alerts, df_summary = analyze_current_run(
        current_stats,
        baseline_data,
        hard_avg_sla=args.hard_avg_sla,
        hard_p95_sla=args.hard_p95_sla,
        hard_error_sla=args.hard_error_sla,
        threshold_factor=args.threshold_factor
    )

    if alerts:
        logging.info("Alerts Detected:")
        for label, issues in alerts:
            for issue in issues:
                logging.info(f"{label}: {issue}")
    else:
        logging.info("No alerts or violations.")

    # Update baseline with new run data
    baseline_data = update_baseline_with_run(baseline_data, current_stats, args.sample_size_per_label)

    # Apply data aging if needed
    baseline_data = apply_data_aging(baseline_data, alpha=args.aging_alpha)

    save_baseline(baseline_data)
    logging.info("Baseline updated.")

    # Export report
    export_report(df_summary, args.output_format, args.report_dir)
    logging.info("Analysis completed successfully.")
