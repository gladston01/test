import psycopg2
import pandas as pd
import os
import json
from datetime import datetime
import argparse

# Configuration
REPORTS_FOLDER = "reports"
HISTORIC_SLA_FILE = "historic_sla.json"

ANOMALY_THRESHOLDS = {
    "MESSAGE_SPIKE_THRESHOLD": 200,
    "TPS_THRESHOLD": 300,
    "CPU_THRESHOLD": 50,
    "MEMORY_THRESHOLD": 50,
    "WORKLIST_THRESHOLD": 800,
    "LOAD_THRESHOLD": 80,
}

# Exclusion lists for specific checks
EXCLUDE_SERVICES_TPS = ["ALL"]
EXCLUDE_SERVICES_MESSAGESPIKE = ["ALL"]

MOVING_AVERAGE_WINDOW = 5
SENSITIVITY_FACTOR = 2


def parse_time(input_time):
    try:
        return datetime.strptime(input_time, "%Y-%m-%d %H:%M:%S")
    except ValueError:
        raise ValueError("Please provide a valid time in 'YYYY-MM-DD HH:MM:SS' format.")


def load_historic_sla():
    if os.path.exists(HISTORIC_SLA_FILE):
        with open(HISTORIC_SLA_FILE, "r") as file:
            return json.load(file)
    return {}


def save_historic_sla(historic_sla):
    with open(HISTORIC_SLA_FILE, "w") as file:
        json.dump(historic_sla, file, indent=4)


def query_pgsql_with_multiple_metrics(start_time, end_time, db_config):
    queries = {
        "durations": """
            SELECT dpbox, leg, domain, service, messagedurations, requestdurations, responsedurations, serverdurations, timestamp
            FROM dpmessagedurations
            WHERE timestamp BETWEEN %s AND %s
        """,
        "counts": """
            SELECT dpbox, leg, domain, service, errormessagecount, requestmessagecount, rresponsemessagecount AS responsemessagecount, timestamp
            FROM dpmessagecounts
            WHERE timestamp BETWEEN %s AND %s
        """,
        "metrics": """
            SELECT dpbox, leg, domain, cpu, worklist, memory, load, timestamp
            FROM dpsystemmetrics
            WHERE timestamp BETWEEN %s AND %s
        """
    }
    try:
        conn = psycopg2.connect(**db_config)
        dataframes = {}
        for key, query in queries.items():
            with conn.cursor() as cursor:
                cursor.execute(query, (start_time, end_time))
                columns = [desc[0] for desc in cursor.description]
                results = cursor.fetchall()
                dataframes[key] = pd.DataFrame(results, columns=columns)
                print(f"Fetched {len(results)} rows for {key} data.")
    except Exception as e:
        print(f"Error querying the database: {e}")
        return None
    finally:
        if conn:
            conn.close()

    return dataframes


def calculate_moving_average(df, column, service, historic_sla):
    """
    Calculates moving average dynamically and includes historical values.
    """
    current_avg = df[column].rolling(window=MOVING_AVERAGE_WINDOW).mean()
    last_avg = current_avg.iloc[-1] if not current_avg.empty else 0

    # Combine historical moving averages
    if service not in historic_sla:
        historic_sla[service] = {}
    if column not in historic_sla[service]:
        historic_sla[service][column] = []
    historic_sla[service][column].append(last_avg)

    # Keep only recent values (window size)
    if len(historic_sla[service][column]) > MOVING_AVERAGE_WINDOW:
        historic_sla[service][column] = historic_sla[service][column][-MOVING_AVERAGE_WINDOW:]

    return last_avg


def detect_anomalies(durations_df, counts_df, metrics_df, historic_sla):
    """
    Detects anomalies across message spikes, TPS, system metrics, and moving average violations.
    """
    anomalies = {"message_spikes": [], "tps_anomalies": [], "system_metric_anomalies": [], "backend_issues": [], "moving_avg_violations": []}

    # Filter out excluded services
    durations_df = durations_df[~durations_df["service"].isin(EXCLUDE_SERVICES_MESSAGESPIKE)]
    counts_df = counts_df[~counts_df["service"].isin(EXCLUDE_SERVICES_TPS)]

    # Moving Average Anomalies
    for service in durations_df["service"].unique():
        service_df = durations_df[durations_df["service"] == service]
        for _, row in service_df.iterrows():
            current_value = row["messagedurations"]
            moving_avg = calculate_moving_average(service_df, "messagedurations", service, historic_sla)

            if current_value > moving_avg + SENSITIVITY_FACTOR * ANOMALY_THRESHOLDS["MESSAGE_SPIKE_THRESHOLD"]:
                anomalies["moving_avg_violations"].append(
                    f"At {row['timestamp']}, {row['service']} in {row['domain']} {row['leg']} exceeded moving average threshold. "
                    f"Value={current_value} ms, Moving Avg={moving_avg:.2f} ms."
                )

    # Backend Issues
    for _, row in durations_df.iterrows():
        if row["serverdurations"] >= 0.8 * row["messagedurations"] > ANOMALY_THRESHOLDS["MESSAGE_SPIKE_THRESHOLD"]:
            anomalies["backend_issues"].append(
                f"At {row['timestamp']}, {row['service']} in {row['domain']} {row['leg']} detected a backend issue. "
                f"Message Duration: {row['messagedurations']} ms, Server Duration: {row['serverdurations']} ms."
            )

    return anomalies


def log_anomalies(anomalies):
    os.makedirs(REPORTS_FOLDER, exist_ok=True)
    for anomaly_type, messages in anomalies.items():
        file_path = os.path.join(REPORTS_FOLDER, f"{anomaly_type}_anomalies.txt")
        with open(file_path, "w") as file:
            if messages:
                file.write("\n".join(messages))
            else:
                file.write(f"No {anomaly_type.replace('_', ' ')} anomalies detected.\n")
    print(f"Anomalies logged to {REPORTS_FOLDER}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Detect anomalies with moving averages, spike detection, and backend checks.")
    parser.add_argument("--start-time", type=str, required=True, help="Start time in 'YYYY-MM-DD HH:MM:SS'.")
    parser.add_argument("--end-time", type=str, required=True, help="End time in 'YYYY-MM-DD HH:MM:SS'.")
    args = parser.parse_args()

    db_config = {
        'host': 'localhost',
        'dbname': 'your_db_name',
        'user': 'your_user',
        'password': 'your_password',
        'port': 5432
    }

    try:
        start_time = parse_time(args.start_time)
        end_time = parse_time(args.end_time)
    except ValueError as e:
        print(e)
        exit(1)

    historic_sla = load_historic_sla()
    dataframes = query_pgsql_with_multiple_metrics(start_time, end_time, db_config)

    if dataframes:
        anomalies = detect_anomalies(dataframes["durations"], dataframes["counts"], dataframes["metrics"], historic_sla)
        save_historic_sla(historic_sla)
        log_anomalies(anomalies)
        print("Anomaly detection complete.")
    else:
        print("No data found for the given time range.")
