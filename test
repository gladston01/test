import psycopg2
import pandas as pd
import json
from datetime import datetime
import argparse
import os
import numpy as np

HISTORIC_SLA_FILE = "historic_sla.json"
REPORTS_FOLDER = "reports"


def parse_time(input_time):
    """
    Parses the input time and ensures it is in the correct format.
    """
    try:
        parsed_time = datetime.strptime(input_time, "%Y-%m-%d %H:%M:%S")
    except ValueError:
        try:
            parsed_time = datetime.fromisoformat(input_time)
        except ValueError:
            print(f"Invalid time format: {input_time}")
            raise ValueError("Please provide a valid time in 'YYYY-MM-DD HH:MM:SS' format.")
    return parsed_time.strftime("%Y-%m-%d %H:%M:%S")


def query_pgsql(start_time, end_time, db_config):
    """
    Queries the PostgreSQL database for services, domains, legs, and their duration metrics.
    """
    sql_query = """
    SELECT 
        service,
        domain,
        leg,
        "timestamp",
        messagedurations,
        requestdurations,
        responsedurations,
        serverdurations
    FROM dpmessagedurations
    WHERE 
        "timestamp" BETWEEN %s AND %s
    """

    try:
        conn = psycopg2.connect(
            host=db_config['host'],
            dbname=db_config['dbname'],
            user=db_config['user'],
            password=db_config['password'],
            port=db_config['port']
        )

        with conn.cursor() as cursor:
            cursor.execute(sql_query, (start_time, end_time))
            columns = [desc[0] for desc in cursor.description]
            results = cursor.fetchall()

        df = pd.DataFrame(results, columns=columns)
    except Exception as e:
        print(f"Error querying the database: {e}")
        return None
    finally:
        if conn:
            conn.close()

    return df


def load_historic_sla():
    """
    Loads historic SLA values from a JSON file.
    """
    if os.path.exists(HISTORIC_SLA_FILE):
        with open(HISTORIC_SLA_FILE, "r") as file:
            return json.load(file)
    return {}


def save_historic_sla(historic_sla):
    """
    Saves historic SLA values to a JSON file.
    """
    def convert_numpy(obj):
        if isinstance(obj, (np.int64, np.int32)):
            return int(obj)
        elif isinstance(obj, (np.float64, np.float32)):
            return float(obj)
        return obj

    with open(HISTORIC_SLA_FILE, "w") as file:
        json.dump(historic_sla, file, indent=4, default=convert_numpy)


def detect_spikes_and_log(df, historic_sla, moving_sla_files, hard_sla=1000, sensitivity_factor=2, num_runs=5):
    """
    Detects spikes where duration values exceed hard SLA, moving average, or standard deviation thresholds.
    Generates separate alert files for each domain and metric, and single Moving SLA files for each metric.
    """
    # Ensure reports folder exists
    os.makedirs(REPORTS_FOLDER, exist_ok=True)

    # Melt the dataframe to bring durations into a single column
    melted_df = df.melt(
        id_vars=["service", "domain", "leg", "timestamp"],
        value_vars=["messagedurations", "requestdurations", "responsedurations", "serverdurations"],
        var_name="duration_type",
        value_name="value"
    )

    moving_sla_breaches = {metric: [] for metric in ["messagedurations", "requestdurations", "responsedurations", "serverdurations"]}
    domain_alerts = {}

    for service in melted_df["service"].unique():
        service_df = melted_df[melted_df["service"] == service]
        for duration_type in ["messagedurations", "requestdurations", "responsedurations", "serverdurations"]:
            duration_df = service_df[service_df["duration_type"] == duration_type]

            if service in historic_sla and duration_type in historic_sla[service]:
                historic_max_values = historic_sla[service][duration_type].get("max_values", [])
                if len(historic_max_values) > 0:
                    moving_avg = sum(historic_max_values) / len(historic_max_values)
                    std_dev = (
                        (sum((x - moving_avg) ** 2 for x in historic_max_values) / len(historic_max_values)) ** 0.5
                    )
                else:
                    moving_avg = 0
                    std_dev = 0
            else:
                moving_avg = 0
                std_dev = 0

            max_value = duration_df["value"].max()
            domain = duration_df.iloc[-1]["domain"]
            leg = duration_df.iloc[-1]["leg"]
            timestamp = duration_df.iloc[-1]["timestamp"]
            alert_key = f"{domain}_{duration_type}"

            # Hard SLA Check
            if max_value > hard_sla:
                if alert_key not in domain_alerts:
                    domain_alerts[alert_key] = []
                domain_alerts[alert_key].append(
                    f"At {timestamp} for {service} in {domain} {leg} {duration_type} SLA exceeded with max value {max_value}"
                )

            # Moving SLA Check
            if max_value > moving_avg + sensitivity_factor * std_dev:
                moving_sla_breaches[duration_type].append(
                    f"At {timestamp} for {service} in {domain} {leg} {duration_type} exceeded Moving SLA of {moving_avg:.2f}ms with max value {max_value}"
                )

            # Sudden Spike Detection
            previous_value = None
            sudden_spike_start = None
            sudden_spike_end = None
            for idx, row in duration_df.iterrows():
                if previous_value is not None and abs(row["value"] - previous_value) > 400:
                    if sudden_spike_start is None:
                        sudden_spike_start = row["timestamp"]
                    sudden_spike_end = row["timestamp"]
                previous_value = row["value"]

            if sudden_spike_start:
                if alert_key not in domain_alerts:
                    domain_alerts[alert_key] = []
                if sudden_spike_start == sudden_spike_end:
                    domain_alerts[alert_key].append(
                        f"Sudden Spike detected for {service} in {domain} {leg} {duration_type} at {sudden_spike_start}, reaching {max_value}ms"
                    )
                else:
                    domain_alerts[alert_key].append(
                        f"Spike detected for {service} in {domain} {leg} {duration_type} between {sudden_spike_start} and {sudden_spike_end}, reaching {max_value}ms"
                    )

            # Update Historic SLA
            if service not in historic_sla:
                historic_sla[service] = {}
            if duration_type not in historic_sla[service]:
                historic_sla[service][duration_type] = {"max_values": []}
            historic_sla[service][duration_type]["max_values"] = (
                historic_sla[service][duration_type]["max_values"][-(num_runs - 1):] + [max_value]
            )

    # Write Alerts to Files by Domain and Metric
    for alert_key, alerts in domain_alerts.items():
        domain, metric = alert_key.split("_")
        alert_file = os.path.join(REPORTS_FOLDER, f"Alerts_{domain}_{metric}.txt")
        with open(alert_file, "w") as f:
            if alerts:
                f.write("Alerts Detected:\n")
                f.write("\n".join(alerts))
                print(f"Alerts for {domain} and {metric} logged to {alert_file}")
            else:
                f.write("No Alerts Detected.\n")
                print(f"No Alerts Detected for {domain} and {metric}.")

    # Write Moving SLA Breaches to Separate Files
    for metric, breaches in moving_sla_breaches.items():
        metric_file = os.path.join(REPORTS_FOLDER, moving_sla_files[metric])
        with open(metric_file, "w") as f:
            if breaches:
                f.write("Moving SLA Breaches Detected:\n")
                f.write("\n".join(breaches))
                print(f"Moving SLA breaches logged to {metric_file}")
            else:
                f.write("No Moving SLA breaches detected.\n")
                print(f"No Moving SLA breaches detected for {metric}.")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Query PostgreSQL and detect SLA breaches in duration metrics.")
    parser.add_argument("--start-time", type=str, required=True, help="Start time in 'YYYY-MM-DD HH:MM:SS' format.")
    parser.add_argument("--end-time", type=str, required=True, help="End time in 'YYYY-MM-DD HH:MM:SS' format.")
    args = parser.parse_args()

    try:
        start_time = parse_time(args.start_time)
        end_time = parse_time(args.end_time)
    except ValueError as e:
        print(e)
        exit(1)

    db_config = {
        'host': 'localhost',
        'dbname': 'your_db_name',
        'user': 'your_user',
        'password': 'your_password',
        'port': 5432
    }

    result_df = query_pgsql(start_time, end_time, db_config)

    if result_df is not None and not result_df.empty:
        historic_sla = load_historic_sla()
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        moving_sla_files = {
            "messagedurations": f"Moving_SLA_messagedurations_{timestamp}.txt",
            "requestdurations": f"Moving_SLA_requestdurations_{timestamp}.txt",
            "responsedurations": f"Moving_SLA_responsedurations_{timestamp}.txt",
            "serverdurations": f"Moving_SLA_serverdurations_{timestamp}.txt",
        }
        detect_spikes_and_log(result_df, historic_sla, moving_sla_files)
        save_historic_sla(historic_sla)
    else:
        print("No data found for the given time range.")
